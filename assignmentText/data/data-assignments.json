[
    {
        "in": 1,
        "titles": "Iterative Pattern (Wallpaper)",
        "section": "Processuality",
        "shortdescription": "Generating a texture or textile design\n",
        "level": "All levels ",
        "tagsstem": "iteration, functional abstraction, coordinates, graphics primitives, graphics transforms, tesselation, symmetries",
        "tagsarts": "pattern, texture, rhythm, shape, curves",
        "learningobjectives": "•  Develop an understanding of drawing functions and the Cartesian coordinate system\n•  Use functional abstraction to encapsulate the code for a modular design element\n•  Gain experience generating high-resolution raster and/or vector designs",
        "brief": "Write code to generate a tiling pattern or textural composition, such as for wallpaper or fabric. Give consideration to aesthetic issues like symmetry, rhythm, color; detail at multiple scales; precise control of shape; and the balance between organic and geometric forms. Your pattern should be designed so that it could be infinitely tiled or extended. Design something you would actually like to put on the walls of your home, or which you could imagine yourself actually wearing. Export your pattern in a high-resolution format, and print it as large as possible. Projects will be critiqued at the pin-board. Remember to sketch first.",
        "variations": "•  Experiment with 2D graphics transformations, such as rotate(), scale(), and mirror reflections.\n•  Use *nested iteration* to develop 2D rhythms or other gridlike visual structures.\n•  Create a *helper function* to encapsulate the way in which a complex visual element (such as a flower, animal, fruit, fleur-de-lys, etc.) is rendered throughout your design.\n•  Strictly using code, reproduce a pre-existing textile or wallpaper design.\n•  Make a kaleidoscope, by incorporating a photographic image or video feed into the pattern.\n•  Have your pattern printed on a real textile, wrapping paper, etc. by an online fabrication service such as Spoonflower, Constrvct, or Print All Over Me. Consider other output devices for realizing your pattern, such as computer-controlled laser cutters, knitting machines, and lace-making services.",
        "advancedstudents": "Think beyond the rectangular grid. Investigate \"wallpaper group\" symmetries; convex uniform tilings; aperiodic tiles; texture synthesis and re-synthesis. Experiment with the use of particle systems for placing elements.",
        "hyphenatemim": 0,
        "makingitmeaningful": "Pattern is the starting point from which we perceive and impose order in the world. The art of visual pattern-making is as old as humankind itself, and countless examples abound, whether functional, decorative or expressive, in forms like mosaics, calendars, tapestry, quilting, jewelry, calligraphy, furniture, architecture, and more. There is an intimate connection between pattern design, visual rhythm, geometry, mathematics, and iterative algorithms; this prompt invites students to hone their understanding of these relationships in purely formal terms. Realizing a design *physically*, especially through digital printing or fabrication in an unusual material or size, can be a watershed moment of synthesis for students who crave \"something real\". ",
        "titlegray": 255,
        "fullpageimage": "wallpaper/todo_spamghetto_2_fullpage.jpg",
        "layout": 11,
        "image": "wallpaper/polya_tallhalf.png\nwallpaper/marrakech_tiles_onethird.jpg\nwallpaper/reas_pill_a_day_onethird.jpg\nwallpaper/gondek_gallifreyan_onethird.png",
        "imageborders": "0\n0\n0\n0",
        "aspirationcaptions": "1. In *Spamgetto* (2009), the Italian design agency Todo presents wallpaper whose design is computationally generated from thousands of spam emails. https://www.flickr.com/photos/todotoit/sets/72157616412434905/\n\n2. George Pólya's illustrations (1924) of the seventeen periodic plane symmetry groups had a profound influence on the algorithmic patternmaking of M.C. Escher.\n\n3. Zellige terracotta tiles in Marrakech (17th century) form edge-to-edge, regular and other tessellations. Photo by Ian Alexander, 2000. From Wikipedia. https://en.wikipedia.org/wiki/Tessellation.\n\n4. Casey Reas's *One Non-Narcotic Pill A Day* (2013) presents a dynamic collage pattern generated from a video recording.\n\n5. Alison Gondek, an introductory programming student at CMU, used JavaScript to create this pattern inspired by the “Circular Gallifreyan” language from *Doctor Who*.",
        "additionalreferences": "\"Aperiodic Tiling\". In Wikipedia. https://en.wikipedia.org/wiki/Aperiodic_tiling\nBailey, David. *David Bailey's World of Escher-like Tessellations*. http://www.tess-elation.co.uk/\nGrünbaum, B., & Shephard, G. C. (1987). *Tilings and patterns*. Freeman.\nJones, O. (1868). *The grammar of ornament*. B. Quaritch.\nSchattschneider, D. \"The Mathematical Side of M. C. Escher,\" in *AMS Notices* 57: 706-718, 2010. http://www.ams.org/journals/notices/201006/rtx100600706p.pdf\n\"Texture Synthesis\". In Wikipedia. https://en.wikipedia.org/wiki/Texture_synthesis\n\"Wallpaper Collection\", Historic New England. http://www.historicnewengland.org/collections-archives-exhibitions/collections-access/highlights/wallpaper\n\"Wallpaper Groups\". In Wikipedia. https://en.wikipedia.org/wiki/Wallpaper_group\n\nAlso see works by: Casey Reas, Marius Watz, Lia, Jonathan McCabe, Mitchell Whitelaw, Tina Frank, Joshua Davis, Dave Bollinger, Holger Lippmann."
    },
    {
        "in": 1,
        "titles": "Face Generator",
        "section": "Processuality",
        "shortdescription": "Drawing parametric faces",
        "level": "All levels",
        "tagsstem": "parametric design, constrained randomness, graphics primitives, variables",
        "tagsarts": "portraiture, physiognomy, character design, puppets, avatars",
        "learningobjectives": "•  Develop familiarity with drawing functions and parametric form\n•  Apply generative principles to expressive character design\n•  Develop skills as a meta-designer",
        "brief": "Write code to create a face design which is parameterized by at least three dimensions of variability, but preferably more. For example, you might have variables that specify the size, position, color, or other visual characteristics of the eyes, nose, and mouth. The variations in these features may be used to alter the face’s expression (happy, sad, angry, etc.); the face’s identity (John, Mary, etc.); and/or the face’s species (cat, monkey, zombie, alien, etc.). Give consideration to *continuous* parameters (such as nose size or eyebrow position), versus *discrete* parameters (such as the presence of piercings, or the number of eyeballs). Your system should generate a new face whenever the user clicks the mouse.",
        "variations": "•  Consider whether your faces are 2D or 3D, and whether they are generated in frontal, profile, and/or three-quarters view.\n•  Give special consideration to controlling the precise *shape* of face parts, such as the curves of the nose, chin, ears, and jowls.\n•  Consider characteristics like skin color, stubble, hairstyle, blemishes, interpupillary distance, facial assymmetry, cephalic index, prognathism, etc.\n•  Use your software to generate a deck of collectible trading cards (like Pokémon or baseball cards), featuring a group of imaginary monsters. Print out the cards.\n•  For visualization students, try using actual multivariate data as the basis for generating new faces, rather than randomness.\n•  Add functionality to your face so that it responds to audio or microphone input. \n•  This project can be used as a follow-up to a preliminary exercise: using code to render a static self-portrait.",
        "advancedstudents": "Create a parameterized facial puppet which is controlled by signals from a real-time face tracker (such as *FaceOSC*). Practice a brief monologue and record a screen-captured video performance with your avatar.",
        "hyphenatemim": 0,
        "makingitmeaningful": "Humans are equipped with an exquisite sensitivity to faces. From infancy we easily recognize faces, and can detect very subtle shifts in facial expressions, often being able to discern the slightest change in mood and sincerity in ways that remain impossible for computers. From faces we are also readily able to identify family resemblances, or \"strangers\" in crowds, and we are transfixed by the ways in which the lines on a face can reveal a person's life history. Kyle McDonald writes that \"faces are so important that the impairment of our face-processing ability is seen as a disorder, called *prosopagnosia*, while unconsciously seeing faces where there are none is an almost universal kind of *pareidolia*.\"\n\nThis assignment draws inspiration from the “Chernoff Face\" data visualization technique, which leverages this sensitivity by using facial features to represent multivariate data. In Chernoff Faces, features such as eyes, ears, mouth and nose represent data according to their shape, size, placement and orientation. Whereas Chernoff (1973) used 18 variables to *synthesize* a face, Paul Ekman’s \"Facial Action Coding System\" uses 46 dimensions to *analyze* a facial expression, each corresponding to the action of a different facial muscle.",
        "titlegray": 0,
        "fullpageimage": "generative_face/moka_faces_1_fullpage.jpg",
        "layout": 20,
        "image": "generative_face/hdh_stranger_visions_quarter.jpg\ngenerative_face/chernoff_faces_quarter.png\ngenerative_face/macawnivore_nose_chart_onethird.png\ngenerative_face/mike_pelletier_onethird.jpg\ngenerative_face/sobecka_perfect_creatures_onethird.jpg",
        "imageborders": "0\n0\n0\n0\n0",
        "aspirationcaptions": "1. Although Matthias Dörfelt’s *Weird Faces* (2012) look hand-drawn, they are entirely expressed by algorithmic rules. http://www.mokafolio.de/works/Weird-Faces\n\n2. In Heather Dewey-Hagborg’s astounding *Stranger Visions* (2012), forensic 3D portraits are computed from found DNA fragments. http://deweyhagborg.com/strangervisions/\n\n3. \"Chernoff faces\" are a type of display in which multivariate data are represented by the shape, size, position and orientation of the parts of the face. From Wikipedia. https://en.wikipedia.org/wiki/Chernoff_face\n\n4. *Nose Chart Reference* (2014) by Macawnivore. http://macawnivore.deviantart.com/art/Nose-Chart-Reference-451870046\n\n5. In Mike Pelletier’s *Parametric Expression*, values that govern the articulation of facial models are pushed beyond their normal limits. http://mikepelletier.nl/Parametric-Expression\n\n6. In Karolina Sobecka’s *All the Universe is Full of the Lives of Perfect Creatures* (2012), the visitor puppeteers an avatar through the movements of their own face. http://www.gravitytrap.com/artwork/perfect-creatures",
        "additionalreferences": "Blas, Zach. *Facial Weaponization Suite*.\nhttps://vimeo.com/57882032\nBravi, Lorenzo et al. *Bla Bla Bla*.\nhttp://goo.gl/2w4CfC\nBorenstein, Greg. *Machine Pareidolia*.\nhttp://urbanhonking.com/ideasfordozens/2012/01/14/machine-pareidolia-hello-little-fella-meets-facetracker/\n\"Computer facial animation\". In Wikipedia. https://en.wikipedia.org/wiki/Computer_facial_animation\nDarwin, Charles. *The Expression of Emotions in Man and Animals*. http://darwin-online.org.uk/\n\"Facial Action Coding System\". In Wikipedia. https://en.wikipedia.org/wiki/Facial_Action_Coding_System\nLevin, Golan and Lieberman, Zachary. *Re:Face*. http://www.flong.com/projects/reface/\nLieberman, Zachary. *Masquelacara*. https://www.instagram.com/p/BDxsVZ0JNpm/\nMcDonald, Kyle. *Face as Interface*. https://github.com/kylemcdonald/AppropriatingNewTechnologies/wiki/Week-2\nMcDonald, Kyle. *FaceOSC*. https://github.com/kylemcdonald/ofxFaceTracker\nMcDonald, Kyle and Castro, Arturo. *Face Substitution*. https://vimeo.com/29348533\nMunari, Bruno. *Design as Art*. (1971)\nPerlin, Ken. *FaceDemo* (Java Applet).\nhttp://mrl.nyu.edu/~perlin/facedemo/"
    },
    {
        "in": 1,
        "titles": "Parametric Alphabet",
        "section": "Processuality",
        "shortdescription": "Structuring letterforms with a common model",
        "level": "Intermediate",
        "tagsstem": "data structures, parametric design, interpolation",
        "tagsarts": "dynamic typography, letterforms",
        "learningobjectives": "•  Use arrays to store geometric data\n•  Apply principles of meta-design to fonts\n•  Conceive and appraise graphical concepts for dynamic typography\n•  Manipulate and/or animate letterforms computationally",
        "brief": "Create a typeface (using any graphic primitives you prefer), such that all of the letters of the alphabet are structured by the same parameters and graphic logic. For example, you might design an alphabet in which every letter is exclusively constructed from 3 arcs, or from 4 rectangles, or from a small grid of squares. After you have designed all of your letters, typeset the entire alphabet on a single canvas so that it can be seen at a glance.\n\nAn essential technical goal is for you to store descriptive parameters for your letters in some kind of array or object-oriented data structure, and then create a single function which renders any requested letter from this data. If you're writing individual functions to draw each letter, you're doing something wrong.",
        "variations": "•  Typeset a carefully chosen word that has a special relationship to your letterforms’ design. \n•  Give your letters inherently unstable properties. Animate your letterforms by deflecting their control points with a sinusoidal wiggle, Perlin noise, or real-time interactivity. \n•  With a single character onscreen, make it possible to animate the transitions between letters, such that any letter can smoothly morph into any other. Pressing a key should initiate an animated transition (of approximately a second's duration) from the previous letter to the next desired letter. *Instructors: for introductory students, it may be helpful to provide a template for transitioning between letterforms.* ",
        "advancedstudents": "•  Consider a design in which letters are traces deposited by moving particles—whose paths are affected by forces from different spatial configurations of attractors and repulsors.\n•  A \"forced aligner\" is a computer program that takes audio files and their transcripts, and returns extremely precise timing information. Using your typeface and a forced aligner (such as *Gentle* by Ochshorn & Hawkins), create time-synced dynamic typography that not only synchronizes perfectly with a speech file, but also responds parametrically to the sound of the speaker's voice.",
        "hyphenatemim": 0,
        "makingitmeaningful": "Extending from Adrian Frutiger's Univers (1954), Donald Knuth's computational MetaFont (1977), and Adobe's \"Multiple Master\" fonts (1994), it has become increasingly common practice to design highly adaptable type *systems* that go far beyond the rigid limits of static type *faces*. Peter Biľak writes: \"Prior to Univers, type designers concerned themselves with the relationships between letters of the same set, how an 'A' is different from a 'B'. Univers goes beyond the quest to design individual letters, attempting instead to create a system of relationships between different sets of shapes which share distinctive parameters.\" \n\nThe creative value of constraints is foregrounded in this prompt. Restricted to designing letterforms with shared parameters, students are compelled to think with modularity, economy and ingenuity about shapes whose variety and complexity they often take for granted. The expressive potential for contingent, interactive and subtly *time-varying* form systems should not be overlooked. Ask each student to name the letters where their structuring pattern succeeds best and fails hardest. Compare and contrast. ",
        "titlegray": 0,
        "fullpageimage": "alphabet/pashenkov_alphabot_fullpage.png",
        "layout": 9,
        "image": "alphabet/huang_typeface_onethird.jpg\nalphabet/lu_alphabet_twothirds.png\nalphabet/cho_typemenot_twothirds.jpg\nalphabet/katsumoto_mojigen_onethird.jpg",
        "imageborders": "0\n0\n0\n0",
        "aspirationcaptions": "1. Nikita Pashenkov's *Alphabot* (2000) is a Transformer-like robot \"that communicates with humans by changing its shape to form characters of the English alphabet.\" Each letter can fold into any other. http://tdctokyo.org/eng/?award=01_nikita-pashenkov\n\n2. Mary Huang's *Typeface2: a Typographic Photobooth* (2010) is a font whose parameters are governed by signals from a real-time face-tracker. http://www.creativeapplications.net/processing/typeface-processing/\n\n3. David Lu's *Letter 3* (2002) presents an interactive alphabet whose letters are formed by manipulating the control points of a single springy curve. Each letter can smoothly transform into any other. http://velluminous.org/portfolio/a/demos/letter3/\n\n4. In Peter Cho's classic *Type Me, Type Me Not* (1997), each letter is constructed from two \"Pac-Man\" filled arcs, and is represented by just 10 numbers. http://acg.media.mit.edu/people/pcho/typemenot/index.html\n\n5. In Yuichiro Katsumoto's *Mojigen & Sujigen* (2016), electromechanical elements shift into place, forming the requested characters. http://www.katsumotoy.com/mojisuji",
        "additionalreferences": "Biľak, Peter. *Designing Type Systems* (2012). https://www.typotheque.com/articles/designing_type_systems\nFlückiger, Michael and Kunz, Nicolas. *LAIKA: a dynamic typeface* (2009). http://laikafont.ch\nHofstadter, Douglas. *Fluid Concepts and Creative Analogies.* (1995).\nJones, C.S. \"What Is Algorithmic Typography?\" (2015). https://crmrkt.com/jyVEO\nKnoth, Christoph. *Computed Type*.\nhttps://vimeo.com/60651938\nKnuth, Donald. \"The Concept of a Meta-Font\" (1982). *Visible Language* 16. http://www.zigzaganimal.be/elements/the-concept-of-metafont.pdf\nLehni, Jürg, et al. *Lego Font Creator* (2001). http://lineto.com/The+Projects/Nicolai.+A+New+Font./\nLehni, Jürg. *Typeface as Programme* (2009). https://www.typotheque.com/articles/typeface_as_programme\nMadsen, Rune. *Printing Code: Typography.* http://printingcode.runemadsen.com/lecture-typography/\nMaeda, John. *Tangram Font* (1993). http://www.maedastudio.com/1997/imda/index.php\nNimoy, Josh. *Robotic Type* (2004). http://cdn.jtn.im/robotictype/\nShim, Kyuha. *Code & Type.* http://code-type.com/about/",
        "notes": "“To create a typeface that is easily malleable in the computational medium, the constituent shapes must be reduced to compact numerical forms.”\n— John Maeda, 2000\n\n“By this art you may contemplate the variation of the 22 letters…”\n— Jorge Luis Borges, The Library of Babel (La biblioteca de Babel), 1941 "
    },
    {
        "in": 1,
        "titles": "Generative Landscape",
        "section": "Processuality",
        "shortdescription": "World-making and terraforming",
        "level": "Intermediate",
        "tagsstem": "probability, randomness, fractals, Perlin noise",
        "tagsarts": "landscape, virtual environment",
        "learningobjectives": "•  Apply principles of generative design to terrain, scenery, and worlds of their own imagination.\n•  Understand how to bias randomness in order to carefully regulate probabilities.",
        "brief": "Write a program which presents an ever-changing, imaginative \"landscape\". Populate your landscape with features that are suitable for your concept: perhaps trees, buildings, vehicles, animals, people, food items, body parts, hairs, seaweed, space junk, zombies etc.\n\nGive consideration to the *depth of variation* in your landscape: after how much time does your landscape become predictable? How might you forestall this as long as possible? How can you generate a landscape which is both consistent and engaging?\n\nConsider: foreground, middleground, and background “layers”; variation at the macro-scale, meso-scale, and micro-scale; natural and human-made features; utopia, dystopia, and heterotopia; and the potential for *surprise*, through the placement of *infrequent* features.",
        "variations": "•  Give consideration to the manner in which the landscape moves past the “camera”. For example, it might appear to scroll by (as if you were looking out the window of a train); or approach from a first-person point of view (as if you were driving, or riding a roller coaster), or slide underneath (as if you were looking out of a glass-bottomed airplane). Consider too a moving or even *roving* camera, capable of rotation as well as translation. \n•  Your project could depict an outside scene, an interior one (such as objects on a conveyor belt), or an altogether dreamlike one. \n•  Consider experimenting with 3D (as in noise terrains); 2D (as in side-scrolling video games); “2.5D” layered spaces; orthographic views; or even nonlinear, non-Cartesian geometries.\n•  Give consideration to *sound*, and the possibility for audiovisual synchronicities (as in *Guitar Hero*).\n•  Make an autonomous creature, vehicle, or other character traverse your landscape.\n•  Implement features in your landscape which grow, evolve or erode over time.\n•  Populate your landscape with one or more of the “three verticals” (people, trees, and buildings): according to Jungian psychology, the defining psychological features of landscapes.",
        "hyphenatemim": 0,
        "makingitmeaningful": "We are a migrant species, instilled with a *wanderlust* that continually clamors for new horizons. Before the modern era of mobility, landscape paintings were often the primary means by which people could visualize and escape, in their minds, to faraway lands. \n\nToday, eight-year olds trade “seeds” for favored Minecraft worlds, and computer-generated environments have become commonplace in video games, where the algorithmic generation of novel landscapes is an economic necessity for inexhaustible play. For the meta-designer and artist-programmer, there is assuredly something godlike about calling forth world upon world. It is probably not a coincidence that the first all-CGI sequence in a feature film depicted the synthesis of an entire planet, in the triumphant “Genesis Sequence” of *Star Trek II* (1982).\n\nThis assignment asks the student to write the rules that bring forth a world from their imagination. But it could equally well ask them to create an accurate computational representation of a very real place—and to generate “more” of it. ",
        "titlegray": 255,
        "fullpageimage": "landscape/daniel_brown_fullpage.jpg",
        "layout": 1,
        "image": "landscape/mandelbrot_voss_quarter.jpg\nlandscape/tarbell_substrate_quarter.jpg\nlandscape/pipkin_mirrorlake_quarter.png\nlandscape/kyttenjanae_lonelyplanets_quarter.png",
        "imageborders": "0\n0\n0\n0",
        "aspirationcaptions": "1. Daniel Brown generates dystopian housing projects in his beautifully lit fractal series, *Travelling by Numbers* (2016).\nhttp://flic.kr/s/aHskyNR2Tz\n\n2. \"Fractional noise\" mountains (c. 1982) developed by Benoît Mandelbrot and Richard F. Voss at IBM were a landmark in mathematical terrain synthesis. http://www.wired.com/2013/01/mandelbrot-images/\n\n3. In Jared Tarbell's classic *Substrate* (2003), simulated urban tectonics arise from elementary principles of accretion, branching, and feedback. http://www.complexification.net/gallery/machines/substrate/\n\n4. Katie Rose Pipkin generates barren flowerpot landscapes in poetic and mysterious browser work, such as *Mirror Lake* (2015). http://katierose.itch.io/mirrorlake\n\n5. Kristyn Janae Solie's *Lonely Planets* (2013) is a stylized 3D terrain that shifts between minimalism and psychedelia. The work was created for Casey Reas' undergraduate course, *Live Cinema through Creative Coding*. http://www.kyttenjanae.com/",
        "additionalreferences": "Akten, Memo and Berio, Daniel. *Bozork Quest* (2013). https://vimeo.com/113106061\nBeddard, Tom. *Surface Detail* (2011). http://sub.blue/surface-detail\nBetts, Tom. *British Countryside Generator* (2014). https://flic.kr/s/aHsjXVXBAE\nBrown, Daniel. *Travelling by Numbers* (2016). http://flic.kr/s/aHskyNR2Tz\nField.io. *Interim Camp* (2009). http://www.field.io/project/interim-camp/\nGeilfus, Simon. *Muon glNext* (2014). https://vimeo.com/108393262\nHart, Vi et al. *Float* (2015). https://vimeo.com/147908916\nHodgin, Robert. *Audio-generated landscape* (2008). https://vimeo.com/2094557\nHoff, Anders. *Isopleth* (2015). http://inconvergent.net/isopleth/\nLemercier, Joanie. *Landforms* (2014). http://joanielemercier.com/landforms/\nMcCormack, Jon. *Morphogenesis Series* (2001). http://jonmccormack.info/\nMcKay, Joe. *Sunset Solitaire* (2010). http://www.joemckaystudio.com/sunset.php\nMolnar, Vera. *Variations St. Victoire* (1989-1996). http://veramolnar.com/\nPlanetside Software. *Terragen* (2008). http://www.planetside.co.uk/\nQuayola, Davide. *Pleasant Places* (2015). http://www.quayola.com/pleasant-places/\nZawada, Jonathan. *Over Time* (2011). http://www.zawada.com.au/2013/01/02/over-time/"
    },
    {
        "in": 1,
        "titles": "Clock",
        "section": "Processuality",
        "shortdescription": "Representing time",
        "level": "All levels",
        "tagsstem": "measurement, data visualization",
        "tagsarts": "time, rhythm, dynamism, animation",
        "learningobjectives": "•  Become acquainted with the history of systems and devices for timekeeping\n•  Devise technologies and graphic concepts for representing time that go beyond conventional methods of visualization and mediation\n•  Refine craft skills in the use of programming to control shape, color, form, and motion",
        "brief": "Design a ‘visual clock’ that displays a novel or unconventional representation of the time. It is not essential that the time of day be literally readable from it, but your clock should appear different at all times of the day, and it should repeat its appearance every 24 hours (or other relevant cycle, if desired).\n\nYou are encouraged to question basic assumptions about how time is mediated and represented. Ponder things like biological time (chronobiology), ultradian and infradian rhythms, solar and lunar cycles, celestial time and sidereal time, decimal time, metric time, geological time, historical time, psychological time, and subjective time. Inform your design by reading about the history of timekeeping systems and devices, and their transformative effects on society.",
        "variations": "•  Feel free to experiment with any of the tools at your disposal, including transparency, color, sound, dynamism, and physical actuation. Reactivity to the cursor is optional. \n•  You're strictly prohibited from using Roman, Arabic, or Chinese numerals. However, you may still make the time literally readable through other means, such as by visualizing numeric bit patterns, or using iteration to present countable graphic elements.\n•  Your code will need to access the current hour(), minute(), second(), and potentially the millis(). However, you could also access the day(), month(), and year(), for a clock which changes over the seasons or human lifespans.\n•  Develop your clock for a portable or wearable device, such as a mobile phone, smart watch, fitness tracker, or other standalone computer with a small display. Consider incorporating data from your device’s other sensors into your design, such as a user’s image, movements, body temperature, heartbeat, etc.\n•  Free yourself from the desktop or laptop screen, and design your clock for a context of your own choosing. If you could place your clock *anywhere*, where would it be? On the side of a building? As a piece of furniture? In someone’s pocket? Or as a digital tattoo? Include a drawing, rendering or other mockup showing your clock as you imagine it *in situ.* ",
        "hyphenatemim": 1,
        "makingitmeaningful": "Attempts to mark time stretch back many thousands of years, with some of the earliest timekeeping technologies being gnomons, sundials, water clocks, and lunar calendars. Even today’s standard representation of time, with hours and minutes divided into 60 parts, is a legacy inherited from the ancient Sumerians, who used a sexagesimal counting system.\n\nThe history of timekeeping is the history of a still unfolding relationship between technological developments and a social pressure for greater precision, accuracy, and synchronization. Every increase in our ability to precisely measure time has had a profound impact on our mastery of science, agriculture, navigation, communications, and as always, warcraft.\n\nDespite the widespread adoption of machinic standards, there are many other ways to understand time. Psychological time contracts and expands with attention; biological cycles affect our moods and behavior; geological or planetary rhythms can span millenia. In the 20th century, Einstein’s theory of relativity further upended our understanding of time, showing that it does not flow in a constant way, but rather in relation to the position from which it is measured, a possibly surprising return to the significance of the observer.",
        "titlegray": 255,
        "fullpageimage": "clock/lee_byron_fullpage_666x522.png",
        "layout": 17,
        "image": "clock/lastclock_twothirds_282x284.jpg\nclock/banded_clock_282x136.jpg\nclock/moniker_all_the_minutes_282x136.png\nclock/standardtime_282x136.jpg\nclock/oscar_diaz_ink_calendar_282x136.jpg",
        "imageborders": "0\n0\n0\n0\n0",
        "aspirationcaptions": "1. Lee Byron's *Center Clock* (2007) presents the time as countable, bouncy circles. Every minute, sixty white “second” circles coalesce to form a new violet “minute” circle, and so on. http://leebyron.com/centerclock/\n\n2. Using a slit-scan technique, Jussi Ängeslevä & Ross Cooper's *Last Clock* (2002) presents traces from a live video feed at three different time scales. http://angesleva.iki.fi/art/last/\n\n3. In Golan Levin's *Banded Clock* (1999), the seconds, minutes and hours of the current time are represented as a series of striated bands. http://www.flong.com/projects/clock/\n\n4. Studio Moniker's *All the Minutes* (2014) is a Twitter bot that retweets mentions of the current time. https://twitter.com/alltheminutes\n\n5. Mark Formanek's *Standard Time* (2003) is a 24-hour performance in which 70 workers constantly construct and deconstruct a large wooden “digital” display of the current time. http://www.standard-time.com/index_en.php\n\n6. *Ink Calendar* by Oscar Diaz (2009) uses the capillary action of ink spreading across paper to display the date.",
        "additionalreferences": "Biegert & Funk. *Qlocktwo.*\nCharity, Mitchell N. *Dot Clock,* 2001.\nDrucker, Johanna. \"Timekeeping\". In *Graphesis: Visual Forms of Knowledge Production.* Harvard, 2014.\nFoer, Joshua. \"A Minor History of Time Without Clocks\". *Cabinet Magazine,* Issue 29, 2008.\nGiffen, Daniel Craig. *Human Clock,* 2001.\nGroom, Amelia. *Time (Documents of Contemporary Art.* MIT Press, 2013.\nHillis, Danny et al. *The Clock of the Long Now.*\nHumansSince1982. *The Clock Clock.*\nLevin, Golan. *Clocks in New Media.* 2016.\nLewis, Richard. \"How Different Cultures Understand Time\". *Business Insider,* 6/1/2014.\nMaeda, John. *12 O’Clocks*, 1996.\nMarclay, Christian. *The Clock,* 2010.\nMorzier, Eric. *Horloge Tactile,* 2005.\nMulder, Sander. *Continue Time,* 2007.\nMulder, Sander. *Pong Clock,* 2005.\nNakamura, Yugo. *Industrious Clock,* 2001.\nPaterson, Katie. *Time Pieces*, 2014. http://www.katiepaterson.org/timepieces/\nSaqoosha. *Sonicode Clock,* 2008.\nTseng, Yen-Wen. *Hand in Hand.*"
    },
    {
        "in": 1,
        "titles": "Experimental Chat",
        "section": "Connectivity",
        "shortdescription": "Interrogating 'togetherness'",
        "level": "Intermediate-advanced",
        "tagsstem": "client-server communication, network protocols, noise, encodings, telepresence",
        "tagsarts": "social media, telematic art, avatars, translation, mapping, multimodal communication",
        "learningobjectives": "•  Understand how to set up a basic webserver.\n•  Gain experience working with a backend web framework.\n•  Design, develop and execute a concept for a social space.",
        "brief": "Develop a multi-user environment that allows people in different locations to communicate with each other in a new way. Your system could facilitate language-based interactions like typing, speaking or reading. Or it could convey other aspects of presence, such as gesture, breath or body dynamics, to explore what Heidegger calls *dasein*, or “being there together”. Carefully consider the *agency* of participants in your system, and the *timing* and *directionality* of their messages. Are the users passive observers, listening to the murmurings of a crowd, or are they contributors to a grand conversation? Is communication *asynchronous*, wherein a user’s traces are encountered by others later? Or is it *synchronous*, allowing for simultaneous participation in a live event? Is your system intended for one-to-one, one-to-many, one-to-many, or many-to-many?",
        "variations": "•  Develop multi-user performance space for theater or performance. How can developing a medium through which participants are connected, augment or modify collaborative practices such as these?",
        "advancedstudents": "•  Create a multiuser environment where one user is an artificial intelligence. You may choose to use natural language processing tools such as Eliza or Rita or create one of your own. \n•  Develop a Multi-User Dungeon (MUD) game which incorporates a chat room. A MUD is an interactive text based game where users collaborate to solve problems and progress the narrative. The first MUD was developed in 1978 and served as one of the earliest online chat rooms, making this variation a rich opportunity to study gaming history. Traditionally these were role-playing games and users would choose to join a specific domain based on common interests.",
        "hyphenatemim": 0,
        "makingitmeaningful": "One of the most evocative experiences offered by digital networks is being connected to someone when physically apart. The creatation of social spaces has characterised networked computation since its inception with some of the earliest chat programs being written in the 1960s and 1970s. The decades that followed the development of the internet brought bulliten board systems, MUDs, text based protocols such as IRC, the public chat rooms of the 1990s and, more recently, instant messanger services and multiuser documents. The magic of first using a multiuser shared document, where changes update in real time is hard to overemphasize to younger students accustomed to such tools.\n\nOf course communication is not confined to language and explorations of multi-modal communication spaces has long concerned media artists. Telepresence works pioneered ways to connect separated audiences prior to the ubiquity of the internet. Kit Galloway and Sherrie Rabinowitz’s Hole in Space (1980) is an early example using video. Multi-user drawing tools like MotionPhone (1990) by Scott Snibbe also shows early experiments developing collaborative creative tools. This assignment could be further focused on gesture or on autonomic body functions.",
        "titlegray": 255,
        "fullpageimage": "chat/hole_in_space_large.jpg",
        "layout": 22,
        "image": "chat/listening_post.jpg\nchat/the_trace.jpg\nchat/motion_phone.jpg\nchat/poop_chat_pro_2.jpg",
        "imageborders": "0\n0\n0\n1",
        "aspirationcaptions": "1. *Hole in Space* by Kit Galloway and Sherrie Rabinowitz (1980) is a pioneering example of telepresence works as it connected audiences between Times Square in New York City and Los Angeles. https://www.youtube.com/watch?v=SyIJJr6Ldg8\n\n2. *Listening Post* (2001) Mark Hansen and Ben Rubin publicly displays phrases from conversations taken from public online chatrooms.\n\n3. *The Trace* (1995) by Rafael Lozano-Hemmer, one participant encounters the presnence of another person located in an identical but separate room. Each is conveyed to the other in light. http://www.lozano-hemmer.com/the_trace.php\n\n4. *Motion Phone* (1995) Scott Snibbe is an early example of a collaborative drawing environment. Users are able to zoom in and draw at multiple scales. http://www.snibbe.com/projects/interactive/motionphone/\n\n5. *Poop Chat Pro*, (2016) Maddy Varner is an asynchonous chat space where messages are deposited as quivering animations for others to find later.",
        "additionalreferences": "Auber, Olivier. *Poietic Generator* (1986). https://en.wikipedia.org/wiki/Poietic_Generator\nBilal, Wafaa. *Domestic Tension* (2007). http://wafaabilal.com/domestic-tension/\nBrucker-Cohen, Jonah. *BumpList* (2003). http://www.bumplist.net\nChat.meatspac.es. *Soledad Penadés*, https://chat.meatspac.es/\nMcDonald, Kyle. *Exhausting a Crowd* (2015). http://www.exhaustingacrowd.com/ \nSam Lavigne and Useless Press.*PckWck* (2015) http://uselesspress.org/things/pckwck/\nExonemo. *The Internet Bedroom* (2015) http://idpw.org/bedroom/\nGage, Zach. *Can We Talk?* (2011). https://vimeo.com/27421540\nHawkins, Max. *Call in the Night* (2013). http://callinthenight.com/\nHoltzer, Jenny. *Truisms* (1997). http://www.medienkunstnetz.de/works/truisms/ \nJilian Mayer *The Sleep Site* (2012) http://aplaceforonlinedreaming.com/\nJingwen Zhu. *Real Me* http://www.jingwen-zhu.com/real-me/ \nKen Goldberg and Joseph Santarromana, (1995) Telegarden. http://www.ieor.berkeley.edu/~goldberg/garden/Ars/\nTwistori, http://twistori.com/ \nStudio Moniker, *Do Not Touch* (2013). http://donottouch.org/\nMoholy-Nagy, László. *Telephone Picture* (1923).",
        "notes": "Considerations: \nRole of the user: viewer or contributor: Are you just listening to the murmurings of a crowd, or able to contribute to it\nAsynchronous vs. Synchronous: participating in a live event, or contributing to a graffiti wall\nTextual or multimodal: Am I typing/speaking/reading, or am i using gesture, breath, my body\n\n\n"
    },
    {
        "in": 1,
        "titles": "Browser Extension",
        "section": "Connectivity",
        "shortdescription": "Lens for the internet",
        "level": "Intermediate",
        "tagsstem": "internet, browser, extension, add-on, network ",
        "tagsarts": "détournement, intervention, interactivity, defamiliarization, modding",
        "learningobjectives": "•  Develop an understanding of protocols for website structure and display\n•  Develop an understanding of Internet browser functionality\n•  Creation of a browser based creative intervention",
        "brief": "A *browser extension* is a software add-on that alters the behavior of a web browser application. Extensions can do things like change how specific online content appears, add additional content layers, redirect a viewer to different URLs, or change browser behaviors. It is a fruitful site for creative intervention in the online realm. Build a browser extension that programmatically alters the appearance of the Internet, or which augments or estranges a viewer's browsing experience in a poetic or critical way. Publish your extension to a public platform like the Chrome store. \n",
        "variations": "Inexperienced students can be introduced to online interventions through a preliminary exercise of modifying website code in the console of the browser. The styling and content of a page can be temporarily edited by changing the CSS or HTML of a page directly or algorithmically using JavaScript. This introductory exercise introduces students to the console, to the basic structure and compilation of a website. This experimentation can scaffold the creation of their extension. \n\nThis brief may be further constrained by narrowing it to one specific intervention, such as a text modification, a word or image find-and-replace, a style-based CSS change, or a content augmentation in which an extra layer of information is added to websites. ",
        "advancedstudents": "•  Make an extension that augments the web through use of data from an external online API or database. \n•  Build an addon that incorporates a multiuser functionality so that it connects or makes the audience aware of others using the same extension. This requires the use of web sockets or a similar.\n• Incorporate a regular expression.",
        "hyphenatemim": 0,
        "makingitmeaningful": "Like a window mediates a view of the physical world, the browser mediates the internet. It is a small but critical piece of infrastructure in the complex system that underlies all of our online activities. Building a creative extension is an opportunity for online intervention at a systems and/or content level. Software studies texts such as the now canonical *Protocol* by Alex Galloway approach the internet from the perspective of infrastructure and protocol, and theoretically scaffold creative interventions through which the browser’s operation and the contruction of the web is revealed.\n \nCreative online interventions that manipulate everyday content build on ideas from conceptual art history. The Situationalists used strategies like detournment and derive to estrange the mundane and everyday experience of life and these are historic precedents to art extensions that modify navigation across the web such as *We See in Every Direction* by Jonas Lund, or that find and replace content like *Rose Colored Window* by Katie Rose Pipkin. Extensions can also be tools for critical art and activism, through strategic edits or by explicating and revealing otherwise obscure relationships by layering additional information. See the Sunlight Foundation’s *Influence Explorer* or the *BookIndy Browse Amazon, Buy Local*.",
        "titlegray": 0,
        "fullpageimage": "extension/decoder.jpg",
        "layout": 24,
        "image": "extension/wesee.jpg\nextension/addart_quarter_282x210.jpg\nextension/newstweek_2.jpg\nextension/us.jpg\n",
        "imageborders": "1\n1\n0\n1",
        "aspirationcaptions": "1. Melanie Hoff's *Decodelia* (2015) uses principles of color theory to render the browser’s content visible only to those wearing red-tinted glasses. https://melanie-hoff.com/DECODELIA/\n\n2. Jonas Lund's *We See in Every Direction* (2013) connects all concurrent users in a collaborative browsing experience. http://ineverydirection.net/\n\n3. Steve Lambert's *Add Art* (2008) automatically replaces online advertisements with art. http://add-art.org/\n\n4. *Newstweek* is a device that enables users to alter news websites by logging into a custom WiFi network. The connected router then injects the modified content to be seen by other users on the network. By Julian Oliver and Daniil Vasiliev (2011), https://julianoliver.com/output/newstweek\n\n5. *Us+* (2013) by Lauren McCarthy and Kyle Mcdonald is an addon for Google facetime, it analyzes conversation dynamics and attempts to offer suggestions for improving the interaction. http://lauren-mccarthy.com/us",
        "additionalreferences": "Burtch, Allison. *Internet Illuminator* (2014). http://www.allisonburtch.net/illuminator/\nDebord, Guy. \"A User’s Guide to Détournement (1).\" (1956).\nGalloway, Alexander. *Protocol: How Control Exists after Decentralization* (2001).\nLavigne, Sam and Tega Brain. *The Intergovernmental Panel on Capitalism* (2015). http://intergovernmentalpaneloncapitalism.org/\nMcNeil, Joanne. *Emotional Labor* (2015). http://www.joannemcneil.com/ga®llery/emotional-labor/\nDavid Nicholls, BookIndy, Browse Amazon, Buy Independent addon. http://bookindy.com/\nPhiffer, Dan and Mushon Zer-Aviv. *ShiftSpace* (2007). https://en.wikipedia.org/wiki/ShiftSpace\nPipkin, Katie Rose. *Rose Colored Window* (2015).\nThe New York Times Special Edition. http://nytimes-se.com/\nSimon, Joel. *FB Graffiti* (2014). http://www.joelsimon.net/fb-graffiti.html\nSunlight Foundation, *Influence Explorer* (2013).\nDarius Kazemi, Ethical Ad Blocker (2015). http://tinysubversions.com/notes/ethical-ad-blocker/\n\n"
    },
    {
        "in": 1,
        "titles": "Software Instrument",
        "section": "Interactivity",
        "shortdescription": "Virtual synaesthesia",
        "level": "Introduction",
        "tagsstem": "gesture, events, libraries, interaction design.",
        "tagsarts": "timbre, translation, rhythm, composition, harmony",
        "learningobjectives": "•  Apply the principles of interaction design to create a software instrument.\n•  Creatively explore ways to connect sound and visuals.\n•  Develop an understanding of event driven programming.",
        "brief": "Create an audiovisual instrument: a tool or toy that responds to a user's actions with tightly coupled sounds and imagery. Interactions with your instrument should yield predictable results. \n\nCarefully consider the interaction design of your instrument: Consider compositional elements such as musical scales, timbre and the variability of these things. Use your instrument to create a brief composition or performance.",
        "variations": "Inexperienced students may find it easier to deal with discrete inputs (like button presses, rather than continuous gestures), and discrete outputs (as in triggering pre-recorded sounds, rather than synthesizing sounds through the modulation of continuous parameters). Consider developing a \"QWERTY instrument\" which is performed through typing actions on a standard computer keyboard. Embed your instrument in a web page and publish it online. If you are using pre-recorded sounds, consider where they are sourced from, how they are visualized, and how this supports a coherent concept. ",
        "advancedstudents": "•  Explore the instrumental possibilities of continuous gesture input using physical sensors, computer vision (such as a face-tracker), or commodity motion capture hardware (such as a Wii, Kinect, or Leap).\n•  Explore the use of arts-oriented machine learning libraries, such as the *Wekinator* or *Gesture Recognition Toolkit*, to enhance the intuitiveness of your input/output mappings.\n•  Divide your instrument into sub-systems (for input, graphics, and sound synthesis) which communicate over OSC. Consider using different development environments for each component (e.g. Max/MSP for sound, etc.).",
        "hyphenatemim": 0,
        "makingitmeaningful": "Synesthesia evocatively couples one sense with another causing people to hear images, taste sounds or see images. Similarly, designing tools for translating between the senses has been an ongoing source of inspiration for creators as seen in the rich history of the color organ. Since the 1700s these instruments have paired sounds and colors, where mechanical systems cause notes to trigger popup panels, open curtains to reveal colored plates or with electricity, turn on colored lights and projections. Along with these elaborate inventions, various artists, scientists and thinkers have developed color scales, systems that structure correspondences between the elements of color (hue, saturation and value) and those of sound (pitch, amplitude, and tone color) see examples by Isaac Newton, D. D. Jameson and A. W. Rimmington. 20th century painters such as Kandinsky have also been captivated by the translation between the visual and the sonic, describing his paintings as compositions or symphonies, comprised of visual “chords”.\nThe recent availability of opensource software and hardware environments has led to further experimentation with new musical interfaces taking the form of synthesiszers, smartphone applications, online composition tools and addons to existing music production suites.",
        "titlegray": 255,
        "fullpageimage": "instrument/levin-lieberman.jpg",
        "layout": 20,
        "image": "instrument/pitaru_282x210.jpg\ninstrument/heap_282x210.jpg\ninstrument/rokeby_282x136.jpg\ninstrument/sufi_282x136.jpg\ninstrument/patatap_282x136.jpg",
        "imageborders": "0\n0\n0\n1\n0",
        "aspirationcaptions": "1. *The Manual Input Workstation* (2004) by Golan Levin and Zachary Lieberman. http://flong.com/projects/miw/\n2. Sonic Wire Sculpture by Amit Pitaru. http://sws.cc/index.html\n3. Imogen Heap‘s software gloves. http://imogenheap.com/home.php?  \n4. David Rokeby’s Very Nervous System (1982-1991). http://www.davidrokeby.com/vns.html\n\n5. Jace Clayton's *Sufi Plug Ins* are a free suite of music software-as-art based on non-Western conceptions of sound and alternative interfaces. http://www.beyond-digital.org/sufiplugins/\n\n6. Patatap by Jono Brandel and Lullatone. http://patatap.com/\n\n\n",
        "additionalreferences": "Rebecca Fiebrink’s Wekinator software tools. http://www.wekinator.org/downloads/\nAlison Parish makes instruments for writing that take on the principles of musical instruments.  http://www.decontextualize.com/projects/nite/\nCory Arcangel, Keyboard Instruments. http://www.coryarcangel.com/things-i-made/2009-003-dreiklavierstucke-op-11"
    },
    {
        "in": 1,
        "titles": "Bot",
        "section": "Connectivity",
        "shortdescription": "Autonomous artistic agent",
        "level": "All levels ",
        "tagsstem": "internet, server, network, automation",
        "tagsarts": "interaction, intervention, social, publishing",
        "learningobjectives": "•  To create a program that publishes content online\n•  Develop an applied understanding of working with a server\n•  Exploration of the API of a online platform.\n",
        "brief": "Create an autonomous software agent, \"a bot\", that posts to an online social media platform. Your bot might explore a particular issue, publish generated content, perform a character, communicate an emotional disposition, interact with users, draw on content from a source text, or promote a specific agenda. It may be a publishing platform, intermittently publishing content to an audience of online subscribers, or it may be programmed to be a social actor, interacting with other users online. Explore the different possible interactions afforded by the API of your chosen platform.\n \n\n\n",
        "variations": "•  Beginners can start by using an online notification service such as *If This Then That* (IFTTT) to automate online interactions with minimal coding.\n•  Create a bot that responds to an online datastream or archive such as that of the New York Times. This will likely require you to navigate another API in addition to that of the social media platform where you are publishing their content. \n•  Tailor your bot so that it reveals something interesting about the API of the online platform you are using. ",
        "hyphenatemim": 0,
        "makingitmeaningful": "Online bots can publish artistic, literary and scientific content to new audiences and offering new opportunities for creative expression and connecting with online audiences. Bot maker Allison Parrish understands practices of poetic or artistic bot making as “PUDG” - procedural, uncreative, data-driven graffetti (Parrish, 2016). Bots can interact with human users, publish real time data in affective ways or act with specific political agendas. Bots are so prevalent that it has been estimated that over half of all internet traffic is generated by these non-human agents. Search engines use bot to crawl the web indexing content and groups of bots, botnets, coordinate action like DDoS attacks to compromise online services.\n\nBots and the underlying artificial intelligence both inspire hope and anxiety, dual sentiments that are palpible in fictional depications like HAL from 2001: A Space Odyssey. Since the 1950s, natural language and AI researchers have developed chatbots like Eliza and Rita, with Eliza being the first to pass the Turing test. This field continues to florish with the release of bots that learn and evolve through interactions with users. These function as assistents, translators and potentially even companions.\n",
        "titlegray": 255,
        "fullpageimage": "bot/fullpage_artassignbot.jpg",
        "layout": 19,
        "image": "bot/ephemerides.jpg\nbot/third_osc.jpg\nbot/pipkin_one_third.jpg\nbot/quarter_bitnik.jpg\nbot/lavigne_quarter.jpg",
        "imageborders": "0\n1\n0\n0\n1 ",
        "aspirationcaptions": "1. Art Assignment Bot\n2. Allison Parrish, Ephemerides.\n3. Reverse OCR by Darius Kazemi is a tumblr bot that takes a random word and draws semi random line until an OCR library recognizes the resultant image as the word. http://reverseocr.tumblr.com/\n4. Katie Rose Pipkin and Loren Schmidt's *Moth Generator* (2015) is a Twitter bot that synthesizes digital images of moths accompanied by generated nomenclatures for each specimen. https://twitter.com/mothgenerator\n5. Random Darknet Shopper by !Mediengruppe Bitnik is an online shopping bot allocated $100 per week for purchases from the Darknet, https://wwwwwwwwwwwwwwwwwwwwww.bitnik.org/r/\n5. CSPAN5 by Sam Lavigne is a Youtube bot that automatically edits videos from the CSPAN channel reducing them to the most frequently stated words and phrases. https://twitter.com/CSPANFive",
        "additionalreferences": "http://p-dpa.net/work/twitter-bot-encyclopedia/\nCSPAN5 by Sam Lavigne\nAllison Parish essay: https://points.datasociety.net/bots-a-definition-and-some-historical-threads-47738c8ab1ce#.z3eub1bpl\nDarius Kazemi, Two Headlines. https://twitter.com/TwoHeadlines\nCongress Edits, https://twitter.com/congressedits\nKatie Rose Pipkin, Bots\nOn Kawara, Pall Thayer, https://twitter.com/On_Kawara\nDarius Kazemi \nNSA Haiku Generator \nArt Assignment Bot, https://twitter.com/artassignbot\nNSA Haiku Generator, http://www.nsahaiku.net/\nPost the Met, Tega Brain https://twitter.com/postthemet\nNYPL Emoji Bot, https://twitter.com/nyplemoji\nCongress Edits, https://twitter.com/congressedits\nBot Summit \nDronestagram, James Bridle\nKeyTweeter, Kyle Mcdonald?\nHow to Sext, https://twitter.com/wikisext?lang=en\nhttps://datasociety.net/blog/2016/02/24/apb-talking-bots/\nTop Gun Bot, Ramsey Nasser. https://twitter.com/555uhz\nDo Not Pay: http://www.donotpay.co.uk/signup.php\nTalking with Machines, http://www.radiolab.org/story/137407-talking-to-machines/\nAmerican Census, https://twitter.com/censusamericans\nSurya Mattu, https://twitter.com/nypostpoetics",
        "notes": "Many good essays on bots here: https://points.datasociety.net/tagged/bots\nThe bottiness of bots.. how they deviate from being human.\nAutomata (but going to talk about this more in virtual creature)\nCould mention processes of generation. Creating a sea of possible outcomes and then selecting the desirable result in those permutations.\n"
    },
    {
        "in": 1,
        "titles": "Augmented Projection",
        "section": "Transmediality",
        "shortdescription": "Illuminated interventions",
        "level": "All levels ",
        "tagsstem": "perspective geometry, computer vision, calibration, optics, simulation",
        "tagsarts": "site, visualization, animation, light-play, graffiti, intervention, defamiliarization, historicization, architectural politics, public space",
        "learningobjectives": "•  Review techniques for working with projectors.\n•  Use code and light to create relationships between real and virtual, physical and digital.\n•  Design and realize a site-specific artistic concept or augmented reality experiment.",
        "brief": "Design a virtual decal for a physical place or object: a time-based projection of graphics or video which is specifically composed to illuminate a chosen site *other* than a blank projection screen. Your imagery might relate to (and elicit new meanings from) otherwise banal architectural features in a wall, such as a power outlet, doorknob, water spigot, elevator buttons, window frame, door jamb, etc. You might design a projection for a specific object, or, if you are able, for a dynamic site such as a vehicle or performer’s body.\n\nSketch some poetic concepts for imagery that relates in a non-arbitrary way to the (geometric, structural, historical, political...) features of your site. Create your concept in code, and project your imagery onto your site or object, taking special care to document it.",
        "variations": "•  Reinterpret a classic arcade game, such as Pong, Snake, Qix, or Pac-Man, so that it uses a specific wall as a playing field. Be sure to incorporate features of the wall into the game.\n•  Create an audiovisual presentation in which features of a real environment are activated by colored projections, synchronized to music.\n•  Create an \"in-situ data visualization\", which projects information *about* a site onto that site. This could take the form of a superimposed “x-ray”, datagraph, political commentary, etc.\n•  If your design requires precise alignment between virtual and physical spaces, you’ll need a tool for projector calibration and keystone correction. Consider Keystonep5 (for Processing), ofxWarp (for openFrameworks) or software like Millumin or TouchDesigner.\n•  Consider using a physics library, such as Box2D, to orchestrate realistic-looking “collisions” between your projected graphics and the actual physical features of your site.\n•  Give consideration to scale. Small is OK.",
        "advancedstudents": "•  Use computer vision to assist with feature detection, projector calibration and alignment.\r\n•  Using simulation principles such as flocking, create an ecosystem of virtual creatures that are projected onto a chosen wall, and which respond to its features.",
        "hyphenatemim": 0,
        "makingitmeaningful": "A wide range of new meanings awaits when an illuminated virtual layer is superimposed onto the physical world. Projection can operate as a *commentary* or critique: an intervention to probe the historical or political dimensions of a building or monument. In other uses, the projection is a site-specific information *visualization,* revealing the internal structures of places or objects, the activities that occur in relation to them, or the resources required in their operation. In performance contexts, projections have created new opportunities for responsive set designs, shadow-play, and choreographies of “digital costumes”: responsive, projected displays that are tightly coupled to a performer’s body and movements. Augmented projections range from spectacular public illusions that make buildings appear to shimmer, through to intimate poetic gestures at the scale of the human face.\n\nThe key challenge is to create a strongly motivated relationship between real and virtual: that is, between the projected light, and the specific person, place or thing on which it is projected. Exemplary works accomplish this through a combination of both rigorous *formal* and *conceptual* engagement with site. Choices about what to project on, and what to project on it, are made simultaneously. ",
        "titlegray": 255,
        "fullpageimage": "augmented_projection/wodiczko_fullpage.jpg",
        "layout": 20,
        "image": "augmented_projection/naimark_displacements_quarter_1.jpg\naugmented_projection/mckay_sunsetsolitaire_quarter.jpg\naugmented_projection/nuage_vert_onethird.jpg\naugmented_projection/jillian_mayer_onethird_6.jpg\naugmented_projection/obermaier_apparition_onethird_1.jpg",
        "imageborders": "0\n0\n0\n0\n0",
        "aspirationcaptions": "1. Krysztof Wodiczko’s *Warsaw Projection* (2005) augmented the façade of the Zachęta National Gallery to focus on the unequal social status of women. Videos of women supporting an entablature inscribed “Artibus” (arts) were presented as architectural Caryatids. http://www.art21.org/artists/krzysztof-wodiczko\n\n2. In Michael Naimark’s *Displacements* (1980), video of a room-sized still life is projected back onto the selfsame scene, painted white. http://www.naimark.net/projects/displacements.html\n\n3. In *Sunset Solitaire* (2007), Joe McKay uses custom software to perform a projection which “matches” the sunset. http://www.joemckaystudio.com/sunset.php\n\n4. *Nuage Vert* (2008), by Helen Evans and Heiko Hansen, visualizes a power plant’s energy consumption with a projected laser outline. https://vimeo.com/17350218\n\n5. In *Scenic Jogging* (2010), Jillian Mayer enters a landscape projected from a moving vehicle. https://youtu.be/uMq9Th3NgGk\n\n6. *Apparition* (2004), by Klaus Obermaier and the Ars Electronica Futurelab, uses computer vision to project imagery onto a moving dancer.   ",
        "additionalreferences": "AntiVJ. *O (Omicron)* (2012). https://vimeo.com/41486619 \nBaker, Chris. *Architectural Integration Tests* (2009). https://vimeo.com/4500145.\nCone, Justin. \"Building Projection Roundup\". *Motionographer.com*, 7/24/2009.\nForman, Eric. *Perceptio Lucis* (2009). http://www.ericforman.com/perceptio-lucis\nGaulon, Benjamin. *DePong* (2003). http://recyclism.com/deponggame.php\nGuidetti, Michael. *Bounce Room* (2010). https://youtu.be/oM5rAQ9NeyQ\nGysin+Vanetti, *Casse* (2006). http://youtu.be/WY5o2plCfKk\nMomeni, Ali and Stephanie Sherman. *Manual for Urban Projection*, 2014. http://c-uir.org/\nMunk, Bradley et al.: *BOX* (2013). http://gmunk.com/BOX-DEMO\nMurano, Francesco. \"Light Works: Experimental Projection Mapping\", 2014. \nSobecka, Karolina. *Wildlife* (2009). http://www.gravitytrap.com/artwork/wildlife\nStudio Moniker. The Designer’s Guide to Over-projection (2013). https://vimeo.com/74294915\nSugrue, Chris. *Delicate Boundaries* (2006). http://csugrue.com/delicateboundaries/\nValbuena, Pablo. *Augmented Sculpture* (2007). https://vimeo.com/34623832\nYesYesNo. *Night Lights* (2011). https://vimeo.com/26778535",
        "notes": "http://cmuems.com/2013/a/assignments/assignment-07/\nhttp://golancourses.net/2015/lectures/interactivity/full-body-interactive-art/\n\npublic space, new audiences, public art. Relationship to the architecture. Geometric, political, architecture and site has an important history. Opportunity for intervention. What makes so many of the eaxmples so poor is that they are purely geometric relationship - this is a great starting point. Making buildings appear to shake and shimmer. But the work obtains special power when it also relates to the history and meaning of site - as wodisko demonstrates. \nPerformance wise- augmented dance projection. \nA wide range of new meanings and experiences can be produced when an illuminated virtual layer is superimposed onto the physical world. In some works, the light projection can operate as a *commentary* or institutional critique: an ephemeral intervention which probes deeper conceptual, historical or political dimensions of a building or monument. In others, the projection is a site-specific information *visualization*, revealing the internal structures of places or objects, the behaviors and activities that occur in relation to them, or the resources required in their operation. In performance and installation contexts, projections have created new opportunities for responsive set designs, shadow-play, and a choreography of \"digital costumes\": responsive, projected displays that are tightly coupled to a performer's body and movements. In scale, augmented projections range from spectacular public illusions that make buildings appear to shimmer and shake, to intimate poetic gestures at the scale of the human face or hand. \n\nThe artist or designer's core challenge is to create a strongly motivated relationship between the real and virtual: that is, between the projected light or imagery, and the specific person, place or thing on which it is projected. Exemplary works of augmented projection accomplish this through a combination of both rigorous *formal* and *conceptual* engagement with site. The creator establishes geometric relationships between the projected image and its canvas (in which the projected image appears to respond or relate to inherent features of the site), but also goes deeper than the site's surface in addressing or amplifying the charged potentials of its history, identity and politics. Choices about what to project on, and what to project on it, are made simultaneously. "
    },
    {
        "in": 1,
        "titles": "Virtual Creature",
        "section": "Interactivity",
        "shortdescription": "Creating artificial life.",
        "level": "Intermediate ",
        "tagsstem": "object-oriented, classes, forces, simulation",
        "tagsarts": "artifical life, motion, animation",
        "learningobjectives": "•  Write functions that animate different types of motion.  \n•  Hone an understanding of object orientated programming.\n•  Use of a class table.\n•  Program an interaction between objects, eg. collision functions.\n",
        "brief": "Your job, Dr. Frankenstein, is to create new life. Program a virtual organism, it could be a sensate creature, a dynamic flock or swarm, an artificial cell-culture, a novel plant or an ecosystem. Your response should algorithmically generate the form and behavior of your new lifeforms. Will they be able to sleep, reproduce, die or eat one another? Consider the relationships between the individuals in your species and develop a corrosponding interplay of simulated forces such as attraction, repulsion or indifference. Your ecosystem may also have abiotic elements or contain more than one environment that exerts different forces. \nGive consideration to the potential for your software to operate as a cultural artifact. Can it attain special relevance through metaphor, commentary or by addressing a real human need or interest?",
        "variations": "•  Write a class for one species of creature and exchange it for creature classes written by other students. Collect at least two other species and implement them in an ecosystem. *This variation can be implemented using a versioning tool such as Github where students can fork each others repositories and implement the code. In this way students gain insight into collaborative development and the importance of practices such as commenting code and communicating process.*",
        "advancedstudents": "•  Create an ecosystem containing a pair or 'dyad' of creatures that respond to each other in some way. \n•  Present your digital ecosystem as an augmented projection, situating it on a novel interface. Can you make your lifeforms respond to the physcial characteristics of your chosen location?",
        "hyphenatemim": 0,
        "makingitmeaningful": "As the myths of Pygmalion, Golem and Frankenstein show, the god-like desire to create artificial life (AL) is as old as humanity itself. This impulse underlies the history of robotics where gestures are mechanically automated like in the Karikuri of Japan or in the early automaton of Europe, like King Philip II’s Praying Monk or the Defecating Duck. More recently, software simulations of life systems allow behaviors and interactions to be programmed and scaled across massive multiagent systems. Many of these system exhibit emergence, where intelligence emerges from the implementation of simple rules across many actors. \nAs either hardware or software, the goal of AL is to create the impression of life. To be distinguished from character design where the focus is on visual appearance, key to this task is to program a creature that is interactive, responsive and contingent on environmental conditions. The results might be companion creatures that stave off loneliness, pets like the Tamagotchi who evoke empathy through their fragility, or mesmerizing digital ecosystems that evolve through time. \n\n",
        "titlegray": 0,
        "fullpageimage": "virtual_creature/Conway_SurveyOfLifeForms_cropped.jpg",
        "layout": 3,
        "image": "virtual_creature/design_io_tallhalf_282x432.jpg\nvirtual_creature/walter_grey_quarter.jpg\nvirtual_creature/karl_sims_quarter.jpg",
        "imageborders": "0\n0\n0\n",
        "aspirationcaptions": "1. This letter was written by mathematician John Conway to Scientific America proposing the concept for Game of Life to be published in their famed mathematical games column. Written in 1970, Conway describes this zero player game and example of celluar automata. It demonstrates the concept of emergence, how complex behaviors can emerge from a simple rule set.\n\n\n2. Design io, Theo and Emily, Connected Worlds, 201X. http://design-io.com/projects/ConnectedWorlds/\n\n3. Walter Grey's Tortises. 1949.\n\n4. Karl Sims, Evolved Virtual Creatures (1994), https://archive.org/details/sims_evolved_virtual_creatures_1994.\n\n",
        "additionalreferences": "Defecating Duck, Jacques de Vaucanson in 1739\nA-Volve (Christa Sommerer & Laurent Mignonneau) [1][2]\nUlrike Gabriel, Terrain 01\nJane Prophet and Dr Gordon Selley's Technosphere was a virtual environment that users could send in creatures that would then effect its evolution.\nReynolds, Craig. Steering behaviors for autonomous characters. http://www.red3d.com/cwr/steer/\nSingle Cell, Golan Levin and friends (2001). \nDouble Cell, Golan Levin and friends (2002).\nInteractive Plant Growing, Laurent Mignonneau & Christa Sommerer (1992).\nLife Writer, Laurent Mignonneau & Christa Sommerer (2006).\nSingle Cell, Golan Levin and friends (2002). Online: http://www.singlecell.org/singlecell.html\nDouble Cell, Golan Levin and friends (2001). Online: http://www.singlecell.org/ \nMorphogenesis Series, Jon Mccormack (2002). Online: http://jonmccormack.info/~jonmc/sa/artworks/morphogenesis-series/\nInteractive Plant Growing, Laurent Mignonneau & Christa Sommerer (1992). Online: http://www.interface.ufg.ac.at/christa-laurent/WORKS/FRAMES/FrameSet.html\nNokia Friends (Toxi + Universal Everything)\nProcessing Monsters (Lukas Vojir et al.)\nSinglecell.org and Doublecell.org (Various)\nCreatures (Lee Byron, CMU’08)\nA Confidence of Vertices (Brandon Morse)\nSniff interactive dog (Karolina Sobecka & Jim George)\nSteering Behaviors (Craig Reynolds)\nBirds! (Robert Hodgin / Flight404)\nHeather Dewey Hagborg, Bugs, https://www.youtube.com/watch?v=rmKMiAze-3Q\nSenster? http://www.senster.com/ihnatowicz/\nWilliam Latham, virtual life\nIn *Upflanze* Susana Soaresm presents a series of hypothetical computer generated plants designed to survive conditions in the future, such as extreme drought, floods and increasingly higher CO2 levels. http://www.susanasoares.com/urpflanze/\nMignonneau, Laurent & Sommerer, Christa. Life Writer (2006): http://www.interface.ufg.ac.at/christa-laurent/WORKS/FRAMES/FrameSet.html ",
        "notes": "From an assignment I gave last year. We were doing nature of code stuff, and i put this lecture together.\nhttps://github.com/tegacodes/Drawing-Seeing-Moving-with-Code/blob/gh-pages/docs/lectures/L-05.md \n\nthis was assignment (based on shiffman's): https://github.com/tegacodes/Drawing-Seeing-Moving-with-Code/blob/gh-pages/docs/project2.md\n\n\nSee: http://cmuems.com/2013/a/assignments/assignment-07/ \n\nThe goal is to create the impression of life and create a responsive, dynamic and interactive system as opposed to \n\nMaking something seem alive.. Distinguish this from character design. Responsive, dynamic, interactive system.\n\nWhy is the product meaningful? Pet, companion, we like to watch living things... tamogochi. care, relationships, communication in non-verbal ways.\n\n\nThe idea of representing life. What could make representing life have more valence than autonomous mechnics.\n\nPurposes to which these characters are put. Why did X make pincchio. Playful companions.\n\nPygmalion, Frankenstein. Golem. Since the early Greeks, humans have obsessed over creating artifical life and life systems. Artificial life began as the mechanical experiments that can be found in histories, where gesture was automated automating are known From canonical examples of computer programs such as Game of Life\n- it begins the moment we have dolls, figurines.\n- Japanese/creatures.\n- non player characters in games and artificial intelligence\n- craig reynolds flocking\n- Automaton\n- Game of life\n- Software/games/interactivity\n- histories of gender.\n- god like\n- Teach about simulation.\n\n- conceptually there is not much difference between a virtual creature and a robot - except this requires electronics, fabrication, \n- the decsion ot make it s ascreen based experience reduced the problem space to that of a software task\n\n- character design. + behavior/\n- less about the appearance but more about designing behavior which is more about contigencies. THis problem is given not as in the design of a visual problem. Conway is purely about designing behavior and interaction. Behavior designs appearance - looks the way it does. \n\nEducators challenge - Put the challenge to the student to \nConsider encouraging the student to have appearance emerge from behaviors. Springs and forces. \nSimulation can be intra to the creature. Ragdoll physics. "
    },
    {
        "in": 1,
        "titles": "Quantified Selfie",
        "section": "Transmediality",
        "shortdescription": "Data driven portraiture",
        "tagsstem": "data, visualisation,",
        "tagsarts": "portraiture, aesthetics, design,",
        "variations": "•  Combine multiple sources of data, in order to establish frameworks within which interesting comparisons can be made, and unexpected insights can be drawn. Think outside the box: What if you somehow contrast your text messages with the collected work of William Shakespeare (5MB)? etc…\n• Create an interactive (as opposed to a static) visualization. Consider how your project can support (one or more of) the core interactive operations of zooming, sorting, filtering and/or querying. Think about how your user can explore your data, in order to experience a large data set in detail. Is interactivity required? Well, not strictly; it is acceptable to generate a high-resolution PDF if that presentation format suited your concept well — or even, a 3D printed sculpture, if you have access to that kind of output. Likewise, I know that some of you are very musically oriented — you are welcome to consider making a information sonification.\n•  Not everything is a timeline, just because you happen to have data with a timestamp. Consider your email, for example. While you can certainly visualize this as a timeline (e.g. the number of emails you sent and received per day), you could also view it as a graph (e.g. the social network of your friends and acquaintances), or as a histogram (e.g. which people you communicate with most, or which words you use most often).\n• Explore some other dataset. Always focus on the subject which captures your imagination and curiosity and the ‘Quantified Selfie’ premise (and the fitness trackers) are offered as a starting point.",
        "hyphenatemim": 0,
        "makingitmeaningful": "Tufte.\nSelf portraits, how do we understand ourselves today, through data.\n2015 - fitness trackers - evolution of this tech from excitement -> reduencancy\nStudents collect an inventory of data on them - email, apps on phones, social media, browser data etc etc.\n\nBuild a device with a novel sensor. (variation? cc measuring device) ",
        "titlegray": 128,
        "fullpageimage": "quantified_selfie/shan_huang_fullpage.png",
        "layout": 1,
        "image": "quantified_selfie/elahi_beds_fullpage.jpg\nquantified_selfie/dear_data_quarter_1.jpg\nquantified_selfie/viegas_themail_quarter.jpg\nquantified_selfie/emin_tent_quarter.jpg",
        "imageborders": "0\n0\n0\n0",
        "additionalreferences": "Nick Felton (http://feltron.com) has produced “Annual Reports”\nShan Huang - Favicon diary.\nAmerican artist Hasan Elahi has been pre-emptively surveilling himself for the FBI \nJen Lowe, One Human HeartBeat\nDear Data, Georgia Lupi\nMeshu - http://rachelbinx.com/Meshu\nSands Fish, Seeing Like a Bank?\nRachel Binx, Wireless project. http://rachelbinx.com/WiFiDiary\nTracy Emin, Every Man I ever slept with\nBrian House, Quotidian Record",
        "notes": "NOTE 5. Give yourself permission to be specific. Sometimes focusing your investigation on a subsetof your data is not only simpler, but can be much more conceptually interesting, than trying to visualize the “whole thing”. For example, instead of visualizing all of your text messages (a study of your texting behavior), what if you only examine the ones you exchanged with a certain person (a study of your relationship)? Another way to think about this is: give yourself permission to have apoint of view.\n\nhttp://golancourses.net/2014/lectures/visualization/data-self-portraiture/\nhttp://golancourses.net/2014/assignments/project-3/"
    },
    {
        "in": 1,
        "titles": "Decorated Skeleton",
        "brief": "For this project, you are asked to write software which creatively interprets, or responds to, the actions of the body. You will develop a computational treatment for motion-capture data. Ideally, both your treatment, and your motion-capture data, will be ‘tightly coupled’ to each other: The treatment will be designed for specific motion-capture data, and the motion-capture data will be intentionally selected or performed for your specific treatment.",
        "variations": "You may work in real-time (interactive), or off-line (animation). \nYou may choose to develop a piece of interactive real-time software, which treats the mocap file as a proxy for data from a live user. Or you may choose to develop a piece of custom animation software, which interprets the mocap file as an input to a lengthy rendering process process. \nYou may use more than one body. \nYour software doesn’t have to be limited to just one body. \nInstead, it could visualize the relationship (or create a relationship) \nbetween two or more bodies (as in Scott Snibbe’s Boundary Functions or). \nIt could visualize or respond to a duet, trio or crowd of people. \nYou may focus on just part of the body. Your software doesn’t need to respond to the entire body; it could focus on interpreting just a single part of the body\nYou may focus on how an environment is affected by the body. Your software doesn’t have to re-skin or visualize the body. Instead, you can develop an environment that is affected by the movements of the body",
        "hyphenatemim": 0,
        "makingitmeaningful": "Ten Creative Opportunities\n\nIt’s important to emphasize that you have a multitude of creative options — well beyond, or alternative to, the initial concept of a “decorated skeleton”. The following ten suggestions, which are by no means comprehensive, are intended to prompt you to appreciate the breadth of the conceptual space you may explore. In all cases, be prepared to justify your decisions.\n\n\nYou may position your ‘camera’ anywhere — including a first-person POV, or with a (user-driven) VR POV. Just because your performance was recorded from a sensor “in front” of you, this does not mean your mocap data must be viewed from the same point of view. Consider displaying your figure in the round, from above, below, or even from the POV of the body itself. (Check out the camera() function in Processing, or the PerspectiveCamera object in three.js, for more ideas. If you’re using three.js, you could also try a WebVR build for Google cardboard.)\nYou may work in 3D or 2D. Although your mocap data represents three-dimensional coordinates, you don’t have to make a 3D scene; for example, you could use your mocap to control an assemblage of 2D shapes. You could even use your body to control two-dimensional typography. (Helpful Processing commands like screenX() and screenY() , or unprojectVector() in three.js, allow you to easily compute the 2D coordinates of a perspectivally-projected 3D point.)\nYou may control the behavior of something non-human. Just because your data was captured from a human, doesn’t mean you must control a human. Consider using your mocap data to puppeteer an animal, monster, plant, or even a non-living object (as in this research on “animating non-humanoid characters with human motion data” from Disney Research).\nYou may record mocap data yourself, or you can use data from an online source. If you’re recording the data yourself, feel free to record a friend who is a performer — perhaps a musician, actor, or athlete. Alternatively, feel free to use data from an online archive or commercial vendor. You may also combine these different sources; for example, you could combine your own awkward performance, with a group of professional backup dancers.\nYou can make software which is analytic or expressive. You are asked to make a piece of software which interprets the actions of the human body. While some of your peers may choose to develop a character animation or interactive software mirror, you might instead elect to create “information visualization” software that presents an analysis of the body’s joints over time. Your software could present comparisons different people making similar movements, or could track the accelerations of movements by a violinist.\nYou may use sound. Feel free to play back sound which is synchronized with your motion capture files. This might be the performer’s speech, or music to which they are dancing, etc. (Check out the Processing Sound Library to play simple sounds, or the PositionalAudio class in three.js, which has the ability to play sounds using 3D-spatialization.)",
        "titlegray": 128,
        "additionalreferences": "Akten, Memo and Quayola, Davide. *Forms* (2012?)\nhttps://vimeo.com/38017188\n\nFranke, Daniel and Kiefer, Cedric. *unnamed soundsculpture* (2012). https://vimeo.com/38840688\n\nLieberman, Zachary. *Walk Cycle / Circle Study* (2016). \nhttps://www.instagram.com/p/BEP-OYCpNqg/\n\nMethod Design, *2016 AICP Sponsor Reel* (2016)\nhttps://vimeo.com/169599296\n\nSchlemmer, Oskar. http://alchetron.com/Oskar-Schlemmer-1278299-W\nhttp://www.deconcrete.org/wp-content/uploads/2012/05/Oskar-Schlemmer_Stelzenl%C3%A4ufer-1927_Bauhaus-Barbican.jpg\n\nUniversal Everything. MTV / Joy x 1000% / Mister Furry\nhttps://vimeo.com/5395669\n\nUniversal Everything. *Walking City* (2014). \nhttps://vimeo.com/85596568"
    },
    {
        "in": 1,
        "titles": "Measuring Device",
        "section": "Transmediality",
        "shortdescription": "Investigating sense and sensibilies",
        "brief": "If you use a suite of files for you sounds and/or images, these must be stored and accessed from an array.",
        "hyphenatemim": 0,
        "makingitmeaningful": "Census historian James C. Scott has pointed out that measurement is a political act. Data artist Natalie Jeremijenko collects measurements in order to prompt evidence-driven discussion. And in the weird world of quantum physics, the term “observer effect” refers to the idea that the very act of measurement itself alters the subject being measured. Each of these is an example of how measurement can be a meaningful act that alters the world and the way we see it.",
        "titlegray": 128,
        "additionalreferences": "Natalie Jeremijenko, Kate Rich, Suicide Box\nProjects by Rafael Lozano-Hemmer: Zero Noon, Tape Recorders, Pulse.\nNatalie Jeremijenko, One Tree(s) [video]\nProjects by Osman Khan: Net Worth, When Laughter Trips…,\nProjects by Matt Kenyon: IED, Spore, Cloud\nExperiential Souvenirs by Bettina Nissen\nFLOAT, an air-quality monitoring kite, by Xiaowei Wang & Deren Guler\niGeigie, DIY radiation monitor by RDTN\nArtistic Content Detector by Arcangel Constantini\n Why The Lion Roars by Anri Sala measures temperature outside a theater. Each ° change prompts a cut to new film.\nfluDoc, a device which shows the presence of Influenza virus using genetically engineered bioluminescent bacteria",
        "notes": "http://cmuems.com/2013/a/assignments/assignment-08/ \nhttp://cmuems.com/2013/a/lectures/lecture-03-sensing/ \nhttps://github.com/tegacodes/EccentricInterfaces/blob/master/practice.md "
    },
    {
        "in": 1,
        "titles": "Parametric Object",
        "section": "Transmediality",
        "learningobjectives": "Recognize and demonstrate awareness of basic generative 3D design principles;\nUse algorithmic techniques to control constructive solid geometry operations;\nCreate generative forms computationally using the OpenSCAD language.\nFormat the documentation of generative objects for optimal sharing with relevant communities of interest.",
        "hyphenatemim": 0,
        "makingitmeaningful": "A parametric object is a “meta-form”: a mutable object, produced by a set of rules, whose properties are governed or articulated by the values of certain variables or parameters. Change the values of the variables, and the form changes in response. Generative design is the activity of authoring such rules (i.e., the code) that generate parametric objects.\nActivities.\n\nIn this exercise you will write OpenSCAD code which generates a parametrically-variable object. Resources for getting started with OpenSCAD have been prepared for you, here.\nTo get your juices flowing, we will first look at a small number of world-class examples of generative objects in art and design, and then some examples of what’s possible in OpenSCAD; for each of these examples, consider what are the “knobs” that make these designs parametric, and what makes these designs memorable or culturally relevant.\nAfter you create your OpenSCAD code, you will be asked to document your project in a variety of ways.",
        "titlegray": 128,
        "additionalreferences": "Jason Salavon Form Study #1 (2004)\nMike Kneupfel’s Keyboard Frequency Sculpture\nMitchell Whitelaw’s Weather Jewelry \nNervous Systems\nRachel Binx and Sha Hwang created Meshu,\nMatthew Epler’s political satire, Grand Old Party.\nNadeem Haidary’s In-Formed Tableware\nphysicalization of portrait data, the Fahz (“your face in a vase”):\nSilk Paviolion Neri Oxman",
        "notes": "http://golancourses.net/2015/lectures/parametric-3d-form/"
    },
    {
        "in": 1,
        "titles": "Drawing Machine",
        "hyphenatemim": 0,
        "titlegray": 128,
        "additionalreferences": "Tim Knowles, Tree Drawings\n"
    },
    {
        "in": 1,
        "titles": "Custom Pixel",
        "section": "Transmediality",
        "hyphenatemim": 0,
        "titlegray": 128
    },
    {
        "in": 1,
        "titles": "Generative Text",
        "hyphenatemim": 0,
        "titlegray": 128
    },
    {
        "in": 1,
        "titles": "One Button Game",
        "hyphenatemim": 0,
        "titlegray": 128
    }
]
